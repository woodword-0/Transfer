{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "#Table list , comma separated list ex. 'Table1,Table2' !!!CASE SENSITIVE!!!\r\n",
        "tables = 'ContactPerson,CustClassificationGroup,CustGroup,CustInvoiceJour,CustInvoiceTrans,CustTable,DataArea,DimensionAttribute,\\\r\n",
        "DimensionAttributeDirCategory,DimensionAttributeValue,DimensionAttributeValueSetItem,DimensionFinancialTag,DirOrganizationName,\\\r\n",
        "DirPartyLocation,DirPartyRelationship,DirPartyTable,DirPersonName,EcoResProduct,EcoResProductTranslation,ForecastModel,ForecastSales,\\\r\n",
        "HcmWorker,InventBatch,InventDim,InventDimCombination,InventLocation,InventSite,InventSum,InventTable,InventTableModule,InventTrans,\\\r\n",
        "InventTransOrigin,Ledger,LineOfBusiness,LogisticsPostalAddress,MarkupTrans,OMOperatingUnit,SalesLine,SalesPool,SalesTable,smmBusRelChainGroup,\\\r\n",
        "smmBusRelSalesDistrictGroup,smmBusRelSegmentGroup,smmBusRelSubSegmentGroup,WHSInventStatus,WMSLocation,CustPackingSlipTrans,CustPackingSlipJour,\\\r\n",
        "Currency,ExchangeRate,ExchangeRateType,ExchangeRateCurrencyPair,SRSAnalysisEnums,BOM,BOMCalcTable,BOMCalcTrans,BOMTable,BOMVersion,\\\r\n",
        "EcoResCategory,EcoResProductMasterDimensionValue,PurchLine,PurchTable,VendGroup,VendPackingSlipJour,VendPackingSlipTrans,VendTable'\r\n",
        "\r\n",
        "\r\n",
        "#tables = 'BOMVersion,EcoResCategory'\r\n",
        "\r\n",
        "#Storage account details where data feed exports are stored\r\n",
        "setupFilesContainer = \"abfss://dynamics365-financeandoperations@mcasandbox01d365foadl.dfs.core.windows.net\"\r\n",
        "setupFilesFolderPath = \"/mca-sandbox-01.sandbox.operations.dynamics.com/\"\r\n",
        "containerName = \"dynamics365-financeandoperations\"\r\n",
        "storageAccount = \"mcasandbox01d365foadl\"\r\n",
        "\r\n",
        "#linked service name to storage where data feed exports are stored\r\n",
        "linkedServiceName = \"FnO_DataLakeStorage\"\r\n",
        "\r\n",
        "#Checkpointing configurations\r\n",
        "CheckpointcontainerName = \"synapse\"\r\n",
        "CheckpointstorageAccount = \"mcainspiredev02syn\"\r\n",
        "ChangeFeedCheckPointRoot = \"ChangeFeedStructuredStreamingCheckpoint\"\r\n",
        "FullCheckPointRoot = \"StructuredStreamingCheckpoint\"\r\n",
        "\r\n",
        "#Delta Databases\r\n",
        "bronzedatabaseName = \"d365_bronze\"\r\n",
        "silverdatabaseName = \"d365_silver\"\r\n",
        "golddatabaseName = \"combined_gold\"\r\n",
        "configsdatabaseName = \"configs\"\r\n",
        "\r\n",
        "#Synapse Storage account details\r\n",
        "targetStorageAccountName = \"mcainspiredev02syn\"\r\n",
        "targetStorageContainerName = \"synapse\"\r\n",
        "rootTargetTablePath = \"synapse/workspaces/mca-inspiredev-02-syn/warehouse/d365_bronze.db/\"\r\n",
        "\r\n",
        "#Pipeline Grouping Name - Core reserved for core ETL for Inspire Platform\r\n",
        "pipelineGroup = \"Core\"\r\n",
        "\r\n",
        "#Initial Setup Flag\r\n",
        "isInitialSetup = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Create configuration tables if not exists\r\n",
        "\r\n",
        "configsSchema = \"CREATE DATABASE IF NOT EXISTS %s\" % (configsdatabaseName)\r\n",
        "spark.sql(configsSchema)\r\n",
        "\r\n",
        "bronzeSchema = \"CREATE DATABASE IF NOT EXISTS %s\" % (bronzedatabaseName)\r\n",
        "spark.sql(bronzeSchema)\r\n",
        "\r\n",
        "silverSchema = \"CREATE DATABASE IF NOT EXISTS %s\" % (silverdatabaseName)\r\n",
        "spark.sql(silverSchema)\r\n",
        "\r\n",
        "goldSchema = \"CREATE DATABASE IF NOT EXISTS %s\" % (golddatabaseName)\r\n",
        "spark.sql(goldSchema)\r\n",
        "\r\n",
        "\r\n",
        "bronzemetatable = \"CREATE TABLE IF NOT EXISTS %s.bronzemetatable ( \\\r\n",
        "SourceTableName string, \\\r\n",
        "SourceStorageAccountName string, \\\r\n",
        "SourceStorageContainerName string, \\\r\n",
        "SourceChangeFeedManifestPath string, \\\r\n",
        "SourceFullTablesManifestPath string, \\\r\n",
        "SourceChangeFeedFilePath string, \\\r\n",
        "SourceFullTablesFilePath string, \\\r\n",
        "CheckpointAccountName string, \\\r\n",
        "CheckpointContainerName string, \\\r\n",
        "ChangeFeedCheckpointPath string, \\\r\n",
        "FullCheckpointPath string, \\\r\n",
        "TargetDatabaseName string, \\\r\n",
        "TargetTableName string, \\\r\n",
        "TargetStorageAccountName string, \\\r\n",
        "TargetStorageContainerName string, \\\r\n",
        "TargetTablePath string, \\\r\n",
        "IsActive int, \\\r\n",
        "IsProcessingComplete int, \\\r\n",
        "PipelineGroup string, \\\r\n",
        "ETLBatchSeriesID bigint) \\\r\n",
        "USING DELTA \\\r\n",
        "PARTITIONED by (PipelineGroup)\" % (configsdatabaseName)\r\n",
        "\r\n",
        "etlbatchseries = \"CREATE TABLE IF NOT EXISTS %s.etlbatchseries (ETLBatchSeriesID bigint, \\\r\n",
        "ETLBatchSeriesCreateDate timestamp, \\\r\n",
        "IsProcessingComplete tinyint, \\\r\n",
        "PipelineGroup string, \\\r\n",
        "IsFullLoad int) \\\r\n",
        "USING DELTA \\\r\n",
        "PARTITIONED by (PipelineGroup)\" % (configsdatabaseName)\r\n",
        "\r\n",
        "silvermeta = \"CREATE TABLE IF NOT EXISTS %s.silvermeta (Database string, \\\r\n",
        "TableName string, \\\r\n",
        "SourceQuery string, \\\r\n",
        "MergeJoinKey string, \\\r\n",
        "IsActive int, \\\r\n",
        "IsProcessingComplete int, \\\r\n",
        "PipelineGroup string, \\\r\n",
        "ETLBatchSeriesID bigint) \\\r\n",
        "USING DELTA \\\r\n",
        "PARTITIONED by (PipelineGroup)\" % (configsdatabaseName)\r\n",
        "\r\n",
        "goldmeta = \"CREATE TABLE IF NOT EXISTS %s.goldmeta (Database string, \\\r\n",
        "TableName string, \\\r\n",
        "SourceQuery string, \\\r\n",
        "MergeJoinKey string, \\\r\n",
        "PartitionKey string, \\\r\n",
        "IsProcessingComplete int, \\\r\n",
        "SeriesOrdinal int, \\\r\n",
        "IsIncrementalOnly int, \\\r\n",
        "PipelineGroup string, \\\r\n",
        "ETLBatchSeriesID bigint) \\\r\n",
        "USING DELTA \\\r\n",
        "PARTITIONED BY (PipelineGroup)\" % (configsdatabaseName)\r\n",
        "\r\n",
        "\r\n",
        "bronzeconstantentity = \"CREATE TABLE IF NOT EXISTS %s.constantentity (TableName string, \\\r\n",
        "FieldName string, \\\r\n",
        "Constant0 string, \\\r\n",
        "FormDisplayName string, \\\r\n",
        "Value string, \\\r\n",
        "Key string, \\\r\n",
        "ETLBatchSeriesID bigint, \\\r\n",
        "DML_Action string) \\\r\n",
        "USING DELTA \" % (bronzedatabaseName)\r\n",
        "\r\n",
        "silverconstantentity = \"CREATE TABLE IF NOT EXISTS %s.constantentity ( \\\r\n",
        "TableName string, \\\r\n",
        "FieldName string, \\\r\n",
        "Constant0 string, \\\r\n",
        "FormDisplayName string, \\\r\n",
        "Value string, \\\r\n",
        "Key string, \\\r\n",
        "ETLBatchSeriesID bigint, \\\r\n",
        "DML_Action string, \\\r\n",
        "ETLIsDeleted int) \\\r\n",
        "USING DELTA \" % (silverdatabaseName)\r\n",
        "\r\n",
        "spark.sql(bronzemetatable)\r\n",
        "spark.sql(etlbatchseries)\r\n",
        "spark.sql(silvermeta)\r\n",
        "spark.sql(goldmeta)\r\n",
        "spark.sql(bronzeconstantentity)\r\n",
        "spark.sql(silverconstantentity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def Convert(string):\r\n",
        "    li = list(string.split(\",\"))\r\n",
        "    return li\r\n",
        "tableList = Convert(tables)\r\n",
        "coreTableList = tableList\r\n",
        "for tb in tableList:\r\n",
        "    try:\r\n",
        "        spark.sql(\"DESCRIBE %s.%s\" % (bronzedatabaseName,tb))\r\n",
        "        spark.sql(\"DESCRIBE %s.%s\" % (silverdatabaseName,tb))\r\n",
        "        tableList.remove(tb)\r\n",
        "            \r\n",
        "    except:\r\n",
        "        a=\"\"\r\n",
        "if pipelineGroup == \"Core\":\r\n",
        "    shiftBronzeConfigToCore = \"UPDATE %s.bronzemetatable \\\r\n",
        "    SET PipelineGroup = 'Core', \\\r\n",
        "        IsActive = 1 \\\r\n",
        "    WHERE lower(TargetTableName) IN (%s)\" % (configsdatabaseName,str(coreTableList).replace('[','').replace(']','').lower())\r\n",
        "\r\n",
        "    shiftSilverConfigToCore = \"UPDATE %s.silvermeta \\\r\n",
        "    SET PipelineGroup = 'Core', \\\r\n",
        "        IsActive = 1 \\\r\n",
        "    WHERE lower(TableName) IN (%s)\" % (configsdatabaseName,str(coreTableList).replace('[','').replace(']','').lower())\r\n",
        "    spark.sql(shiftBronzeConfigToCore)\r\n",
        "    spark.sql(shiftSilverConfigToCore)\r\n",
        "if len(tableList) == 0:\r\n",
        "    mssparkutils.notebook.exit(\"Tables already exist in bronze and silver. Skipping import and exiting notebook execution. No work performed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def left(s, amount):\r\n",
        "    return s[:amount]\r\n",
        "\r\n",
        "def right(s, amount):\r\n",
        "    return s[-amount:]\r\n",
        "\r\n",
        "abfssString = \"abfss://\" + containerName + \"@\" + storageAccount + \".dfs.core.windows.net\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "from pyspark.sql import DataFrame\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "final_df = spark.sql(\"SELECT 'EMPTY' as Empty\")\r\n",
        "\r\n",
        "mssparkutils.fs.unmount(\"/Tables_mnt\")\r\n",
        "\r\n",
        "mssparkutils.fs.mount(\r\n",
        "    setupFilesContainer,\r\n",
        "    \"/Tables_mnt\",\r\n",
        "    {\"linkedService\" : linkedServiceName}\r\n",
        ")\r\n",
        "\r\n",
        "jbID = mssparkutils.env.getJobId()\r\n",
        "path='/synfs/' + jbID + '/Tables_mnt' + setupFilesFolderPath\r\n",
        "replacePath='/synfs/' + jbID + '/Tables_mnt'\r\n",
        "result = []\r\n",
        "fullResult = []\r\n",
        "for x in os.walk(path):\r\n",
        "    for y in glob.glob(os.path.join(x[0], '*manifest.cdm.json')):\r\n",
        "        if \"ChangeFeed\" not in y and \"/Custom/\" not in y:\r\n",
        "            fullResult.append(y)          \r\n",
        "        if \"ChangeFeed\" in y:\r\n",
        "            result.append(y)     \r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "df = spark.createDataFrame(result, StringType())\r\n",
        "dataCollect = df.collect()\r\n",
        "\r\n",
        "schema = StructType([ \\\r\n",
        "    StructField(\"Account\",StringType(),True), \\\r\n",
        "    StructField(\"Manifest\",StringType(),True), \\\r\n",
        "    StructField(\"Entity\",StringType(),True)])\r\n",
        "\r\n",
        "dfExists = 0\r\n",
        "\r\n",
        "\r\n",
        "for row in dataCollect:   \r\n",
        "    configPath = row['value'].replace(replacePath,setupFilesContainer)\r\n",
        "    \r\n",
        "    path = configPath[::-1]\r\n",
        "    startString = int(path.find(\"/\")+ 1)\r\n",
        "    stepString = 10\r\n",
        "    path = path[path.find(\"/\")+ 1:len(path)]\r\n",
        "    path = path[::-1]\r\n",
        "\r\n",
        "    df1 = spark.read.json(configPath, multiLine=True)\r\n",
        "    i=0\r\n",
        "    try:\r\n",
        "        for subManifest in df1.select(df1['entities']).take(1):\r\n",
        "            while i<len(subManifest[0]):\r\n",
        "                if i > 1000:\r\n",
        "                    break\r\n",
        "                entityName =subManifest[0][i]['entityName']\r\n",
        "                if  len(tableList) > 0:\r\n",
        "                    if entityName in tableList:\r\n",
        "                        data = [(setupFilesContainer.replace(\"abfss://\",\"\").replace('dynamics365-financeandoperations@',''),\"dynamics365-financeandoperations\"+configPath.replace(setupFilesContainer,''), entityName)]\r\n",
        "                        if dfExists == 0:\r\n",
        "                            dfManifest = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                            dfExists = 1\r\n",
        "                        else:\r\n",
        "                            dfManifestAppend = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                            dfManifest = dfManifest.union(dfManifestAppend)\r\n",
        "                else:\r\n",
        "                    data = [(setupFilesContainer.replace(\"abfss://\",\"\").replace('dynamics365-financeandoperations@',''),\"dynamics365-financeandoperations\"+configPath.replace(setupFilesContainer,''), entityName)]\r\n",
        "                    if dfExists == 0:\r\n",
        "                        dfManifest = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                        dfExists = 1\r\n",
        "                    else:\r\n",
        "                        dfManifestAppend = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                        dfManifest = dfManifest.union(dfManifestAppend)\r\n",
        "                i=i+1\r\n",
        "            \r\n",
        "    except :\r\n",
        "        print(\"Following filepath does not contain submanifest values. Skipping. \" + configPath)\r\n",
        "    if i>1000:\r\n",
        "        raise Exception(\"Error: Submanifest list exceeded 1000 entityName values.\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "\r\n",
        "dfExists = 0\r\n",
        "\r\n",
        "def left(s, amount):\r\n",
        "    return s[:amount]\r\n",
        "\r\n",
        "def right(s, amount):\r\n",
        "    return s[-amount:]\r\n",
        "\r\n",
        "schema = StructType([ \\\r\n",
        "    StructField(\"SourceTableName\",StringType(),True), \\\r\n",
        "    StructField(\"SourceStorageAccountName\",StringType(),True), \\\r\n",
        "    StructField(\"SourceStorageContainerName\",StringType(),True), \\\r\n",
        "    StructField(\"SourceChangeFeedManifestPath\",StringType(),True), \\\r\n",
        "    StructField(\"SourceFullTablesManifestPath\",StringType(),True), \\\r\n",
        "    StructField(\"SourceChangeFeedFilePath\",StringType(),True), \\\r\n",
        "    StructField(\"SourceFullTablesFilePath\",StringType(),True), \\\r\n",
        "    StructField(\"CheckpointAccountName\",StringType(),True), \\\r\n",
        "    StructField(\"CheckpointContainerName\",StringType(),True), \\\r\n",
        "    StructField(\"ChangeFeedCheckpointPath\",StringType(),True), \\\r\n",
        "    StructField(\"FullCheckpointPath\",StringType(),True),\r\n",
        "    StructField(\"TargetDatabaseName\",StringType(),True),\r\n",
        "    StructField(\"TargetTableName\",StringType(),True),\r\n",
        "    StructField(\"TargetStorageAccountName\",StringType(),True),\r\n",
        "    StructField(\"TargetStorageContainerName\",StringType(),True),\r\n",
        "    StructField(\"TargetTablePath\",StringType(),True), \\\r\n",
        "    StructField(\"IsActive\",IntegerType(),True), \\\r\n",
        "    StructField(\"IsProcessingComplete\",IntegerType(),True)])\r\n",
        "dataChangeFeed=[]\r\n",
        "\r\n",
        "dfManifestCollect = dfManifest.collect()\r\n",
        "\r\n",
        "for colManifest in dfManifestCollect:\r\n",
        "    #storage = colManifest[\"Account\"]\r\n",
        "    manifestPath = colManifest[\"Manifest\"]\r\n",
        "    entity = colManifest[\"Entity\"]\r\n",
        "    \r\n",
        "    manifestPath = abfssString + right(manifestPath,(len(manifestPath)-len(containerName)))\r\n",
        "    \r\n",
        "    revmanifestPath = manifestPath[len(manifestPath)::-1]\r\n",
        "    ordVal = revmanifestPath.find(\"/\")\r\n",
        "    substringrevmanifestPath = right(revmanifestPath,(len(revmanifestPath) -ordVal - 1))\r\n",
        "    substringmanifestPath = substringrevmanifestPath[len(substringrevmanifestPath)::-1]\r\n",
        "    \r\n",
        "    sourceChangeFeedManifestPath =  right(manifestPath,(len(manifestPath)-(len(abfssString)+1)))\r\n",
        "    df1 = spark.read.json(manifestPath, multiLine=True)\r\n",
        "    entityPaths = df1.select(\"entities\").limit(1).collect()[0][0]\r\n",
        "    for entity in entityPaths:\r\n",
        "        if  len(tableList) > 0:\r\n",
        "            if entity[\"entityName\"] in tableList:\r\n",
        "                entityName = entity[\"entityName\"]\r\n",
        "                #sourceChangeFeedFilePath = substringmanifestPath + \"/\" + entityName\r\n",
        "                sourceChangeFeedFilePath = (substringmanifestPath + \"/\" + entityName).replace(abfssString,\"\")\r\n",
        "                targetTablePath = rootTargetTablePath + entityName.lower()\r\n",
        "                entityPath = entity[\"entityPath\"]\r\n",
        "                targetTableName = entityName\r\n",
        "                entityManifestPath = left(entityPath,(len(entityPath) - (len(entityName)+1)))\r\n",
        "                entityFilePath = substringmanifestPath + \"/\"+ entityName\r\n",
        "                dataChangeFeed.append((entityName,storageAccount,containerName,sourceChangeFeedManifestPath,\"\",right(sourceChangeFeedFilePath,len(sourceChangeFeedFilePath)-1),\"\",CheckpointstorageAccount,CheckpointcontainerName,ChangeFeedCheckPointRoot+\"/\"+entityName,FullCheckPointRoot+\"/\"+entityName,bronzedatabaseName,targetTableName,targetStorageAccountName,targetStorageContainerName,targetTablePath,1,1))\r\n",
        "        else:\r\n",
        "            entityName = entity[\"entityName\"]\r\n",
        "            #sourceChangeFeedFilePath = substringmanifestPath + \"/\" + entityName\r\n",
        "            sourceChangeFeedFilePath = (substringmanifestPath + \"/\" + entityName).replace(abfssString,\"\")\r\n",
        "            targetTablePath = rootTargetTablePath + entityName.lower()\r\n",
        "            entityPath = entity[\"entityPath\"]\r\n",
        "            targetTableName = entityName\r\n",
        "            entityManifestPath = left(entityPath,(len(entityPath) - (len(entityName)+1)))\r\n",
        "            entityFilePath = substringmanifestPath + \"/\"+ entityName\r\n",
        "            dataChangeFeed.append((entityName,storageAccount,containerName,sourceChangeFeedManifestPath,\"\",right(sourceChangeFeedFilePath,len(sourceChangeFeedFilePath)-1),\"\",CheckpointstorageAccount,CheckpointcontainerName,ChangeFeedCheckPointRoot+\"/\"+entityName,FullCheckPointRoot+\"/\"+entityName,bronzedatabaseName,targetTableName,targetStorageAccountName,targetStorageContainerName,targetTablePath,1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS %s.bronzeMeta2\" % (configsdatabaseName))\r\n",
        "rdd = sc.parallelize(dataChangeFeed)\r\n",
        "dfBronzeConfigurations = sqlContext.createDataFrame(rdd, schema)  \r\n",
        "dfBronzeConfigurations.createOrReplaceTempView(\"vwBronzeChangeFeed\")\r\n",
        "dfBronzeConfigurations = spark.sql(\"SELECT DISTINCT * FROM vwBronzeChangeFeed\")\r\n",
        "configsTableName = configsdatabaseName + \".bronzeMeta2\"\r\n",
        "dfBronzeConfigurations.write.mode(\"overwrite\").format(\"delta\").saveAsTable(configsTableName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "df = spark.createDataFrame(fullResult, StringType())\r\n",
        "dataCollect = df.collect()\r\n",
        "\r\n",
        "schema = StructType([ \\\r\n",
        "    StructField(\"Account\",StringType(),True), \\\r\n",
        "    StructField(\"Manifest\",StringType(),True), \\\r\n",
        "    StructField(\"Entity\",StringType(),True)])\r\n",
        "\r\n",
        "dfExists = 0\r\n",
        "\r\n",
        "\r\n",
        "for row in dataCollect:   \r\n",
        "    configPath = row['value'].replace(replacePath,setupFilesContainer)\r\n",
        "    \r\n",
        "    path = configPath[::-1]\r\n",
        "    startString = int(path.find(\"/\")+ 1)\r\n",
        "    stepString = 10\r\n",
        "    path = path[path.find(\"/\")+ 1:len(path)]\r\n",
        "    path = path[::-1]\r\n",
        "    i=0\r\n",
        "    df1 = spark.read.json(configPath, multiLine=True)    \r\n",
        "    try:\r\n",
        "        for subManifest in df1.select(df1['entities']).take(1):\r\n",
        "            while i<len(subManifest[0]):\r\n",
        "                if i > 1000:\r\n",
        "                    break\r\n",
        "                entityName =subManifest[0][i]['entityName']\r\n",
        "                if  len(tableList) > 0:\r\n",
        "                    if entityName in tableList:\r\n",
        "                        data = [(setupFilesContainer.replace(\"abfss://\",\"\").replace('dynamics365-financeandoperations@',''),\"dynamics365-financeandoperations\"+configPath.replace(setupFilesContainer,''), entityName)]\r\n",
        "                        if dfExists == 0:\r\n",
        "                            dfManifest = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                            dfExists = 1\r\n",
        "                        else:\r\n",
        "                            dfManifestAppend = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                            dfManifest = dfManifest.union(dfManifestAppend)\r\n",
        "                else:\r\n",
        "                    data = [(setupFilesContainer.replace(\"abfss://\",\"\").replace('dynamics365-financeandoperations@',''),\"dynamics365-financeandoperations\"+configPath.replace(setupFilesContainer,''), entityName)]\r\n",
        "                    if dfExists == 0:\r\n",
        "                        dfManifest = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                        dfExists = 1\r\n",
        "                    else:\r\n",
        "                        dfManifestAppend = spark.createDataFrame(data=data,schema=schema)\r\n",
        "                        dfManifest = dfManifest.union(dfManifestAppend)\r\n",
        "                i=i+1\r\n",
        "            \r\n",
        "    except :\r\n",
        "        print(\"Following filepath does not contain submanifest values. Skipping. \" + configPath)\r\n",
        "    if i>1000:\r\n",
        "        raise Exception(\"Error: Submanifest list exceeded 1000 entityName values.\")       \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "\r\n",
        "dfExists = 0\r\n",
        "\r\n",
        "def left(s, amount):\r\n",
        "    return s[:amount]\r\n",
        "\r\n",
        "def right(s, amount):\r\n",
        "    return s[-amount:]\r\n",
        "\r\n",
        "schema = StructType([ \\\r\n",
        "    StructField(\"SourceTableName\",StringType(),True), \\\r\n",
        "    StructField(\"SourceStorageAccountName\",StringType(),True), \\\r\n",
        "    StructField(\"SourceStorageContainerName\",StringType(),True), \\\r\n",
        "    StructField(\"SourceChangeFeedManifestPath\",StringType(),True), \\\r\n",
        "    StructField(\"SourceFullTablesManifestPath\",StringType(),True), \\\r\n",
        "    StructField(\"SourceChangeFeedFilePath\",StringType(),True), \\\r\n",
        "    StructField(\"SourceFullTablesFilePath\",StringType(),True), \\\r\n",
        "    StructField(\"CheckpointAccountName\",StringType(),True), \\\r\n",
        "    StructField(\"CheckpointContainerName\",StringType(),True), \\\r\n",
        "    StructField(\"ChangeFeedCheckpointPath\",StringType(),True), \\\r\n",
        "    StructField(\"FullCheckpointPath\",StringType(),True), \\\r\n",
        "    StructField(\"TargetDatabaseName\",StringType(),True), \\\r\n",
        "    StructField(\"TargetTableName\",StringType(),True), \\\r\n",
        "    StructField(\"TargetStorageAccountName\",StringType(),True), \\\r\n",
        "    StructField(\"TargetStorageContainerName\",StringType(),True), \\\r\n",
        "    StructField(\"TargetTablePath\",StringType(),True), \\\r\n",
        "    StructField(\"IsActive\",IntegerType(),True), \\\r\n",
        "    StructField(\"IsProcessingComplete\",IntegerType(),True)])\r\n",
        "\r\n",
        "\r\n",
        "dfManifestCollect = dfManifest.collect()\r\n",
        "data=[]\r\n",
        "for colManifest in dfManifestCollect:\r\n",
        "    storage = colManifest[\"Account\"]\r\n",
        "    manifestPath = colManifest[\"Manifest\"]\r\n",
        "    entity = colManifest[\"Entity\"]\r\n",
        "    \r\n",
        "    manifestPath = abfssString + right(manifestPath,(len(manifestPath)-len(containerName)))\r\n",
        "    #filePath = right(manifestPath,(len(manifestPath)-len(containerName)))\r\n",
        "    revmanifestPath = manifestPath[len(manifestPath)::-1]\r\n",
        "    ordVal = revmanifestPath.find(\"/\")\r\n",
        "    substringrevmanifestPath = right(revmanifestPath,(len(revmanifestPath) -ordVal - 1))\r\n",
        "    substringmanifestPath = substringrevmanifestPath[len(substringrevmanifestPath)::-1]\r\n",
        "\r\n",
        "    sourceFullTableManifestPath =  right(manifestPath,(len(manifestPath)-(len(abfssString)+1)))\r\n",
        "    df1 = spark.read.json(manifestPath, multiLine=True)\r\n",
        "    entityPaths = df1.select(\"entities\").limit(1).collect()[0][0]\r\n",
        "    for entity in entityPaths:\r\n",
        "        if len(tableList) > 0:\r\n",
        "            if entity[\"entityName\"] in tableList:\r\n",
        "                entityName = entity[\"entityName\"]\r\n",
        "                sourceFullTableFilePath = (substringmanifestPath + \"/\" + entityName).replace(abfssString,\"\")\r\n",
        "                targetTablePath = rootTargetTablePath + entityName.lower()\r\n",
        "                entityPath = entity[\"entityPath\"]\r\n",
        "                targetTableName = entityName\r\n",
        "                entityManifestPath = left(entityPath,(len(entityPath) - (len(entityName)+1)))\r\n",
        "                #entityConfigurationPath = substringmanifestPath + \"/\"+ entityManifestPath\r\n",
        "                entityFilePath = substringmanifestPath + \"/\"+ entityName\r\n",
        "                \r\n",
        "                data.append((entityName,storageAccount,containerName,\"\",sourceFullTableManifestPath,\"\",right(sourceFullTableFilePath,len(sourceFullTableFilePath)-1),CheckpointstorageAccount,CheckpointcontainerName,ChangeFeedCheckPointRoot+\"/\"+entityName,FullCheckPointRoot+\"/\"+entityName,bronzedatabaseName,targetTableName,targetStorageAccountName,targetStorageContainerName,targetTablePath,1,1))\r\n",
        "        else:\r\n",
        "            entityName = entity[\"entityName\"]\r\n",
        "            sourceFullTableFilePath = (substringmanifestPath + \"/\" + entityName).replace(abfssString,\"\")\r\n",
        "            targetTablePath = rootTargetTablePath + entityName.lower()\r\n",
        "            entityPath = entity[\"entityPath\"]\r\n",
        "            targetTableName = entityName\r\n",
        "            entityManifestPath = left(entityPath,(len(entityPath) - (len(entityName)+1)))\r\n",
        "            #entityConfigurationPath = substringmanifestPath + \"/\"+ entityManifestPath\r\n",
        "            entityFilePath = substringmanifestPath + \"/\"+ entityName\r\n",
        "            \r\n",
        "            data.append((entityName,storageAccount,containerName,\"\",sourceFullTableManifestPath,\"\",right(sourceFullTableFilePath,len(sourceFullTableFilePath)-1),CheckpointstorageAccount,CheckpointcontainerName,ChangeFeedCheckPointRoot+\"/\"+entityName,FullCheckPointRoot+\"/\"+entityName,bronzedatabaseName,targetTableName,targetStorageAccountName,targetStorageContainerName,targetTablePath,1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS %s.bronzeMeta3\" % (configsdatabaseName))\r\n",
        "rdd = sc.parallelize(data)\r\n",
        "dfFullBronzeConfigurations = sqlContext.createDataFrame(rdd, schema)  \r\n",
        "dfFullBronzeConfigurations.createOrReplaceTempView(\"vwBronzeFull\")\r\n",
        "dfFullBronzeConfigurations = spark.sql(\"SELECT DISTINCT * FROM vwBronzeFull\")\r\n",
        "configsTableName = configsdatabaseName + \".bronzeMeta3\"\r\n",
        "dfFullBronzeConfigurations.write.mode(\"overwrite\").format(\"delta\").saveAsTable(configsTableName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "sqlCommand = \"SELECT A.SourceTableName,A.SourceStorageAccountName,A.SourceStorageContainerName,B.SourceChangeFeedManifestPath,A.SourceFullTablesManifestPath,B.SourceChangeFeedFilePath,A.SourceFullTablesFilePath,A.CheckpointAccountName,A.CheckpointContainerName,A.ChangeFeedCheckpointPath,A.FullCheckpointPath,A.TargetDatabaseName,A.TargetTableName,A.TargetStorageAccountName,A.TargetStorageContainerName,A.TargetTablePath, 1 AS IsActive, A.IsProcessingComplete,'%s' AS PipelineGroup, CAST(0 AS BIGINT) AS ETLBatchSeriesID FROM  %s.bronzeMeta3 A INNER JOIN %s.bronzeMeta2 B ON A.SourceTableName = B.SourceTableName\" % (pipelineGroup,configsdatabaseName,configsdatabaseName)\r\n",
        "dfConfigsBronzeMetaTable = spark.sql(sqlCommand)\r\n",
        "dfConfigsBronzeMetaTable.createOrReplaceTempView(\"vwBronzeMetaTable\")\r\n",
        "configsTableName = configsdatabaseName + \".BronzeMetaTable\"\r\n",
        "\r\n",
        "sqlMerge = \"MERGE INTO %s T \\\r\n",
        "USING vwBronzeMetaTable S ON T.TargetTableName = S.TargetTableName AND T.PipelineGroup = S.PipelineGroup \\\r\n",
        "WHEN MATCHED THEN UPDATE SET * \\\r\n",
        "WHEN NOT MATCHED THEN INSERT *\" % (configsTableName)\r\n",
        "spark.sql(sqlMerge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if len(tableList) > 0:\r\n",
        "    sqlCommand = \"SELECT DISTINCT * FROM %s WHERE IsActive = 1 AND PipelineGroup = '%s' AND lower(TargetTableName) IN (%s) ORDER BY SourceTableName\" % (configsTableName,pipelineGroup,str(tableList).replace('[','').replace(']','').lower())\r\n",
        "else:\r\n",
        "    sqlCommand = \"SELECT DISTINCT * FROM %s WHERE IsActive = 1 AND PipelineGroup = '%s' ORDER BY SourceTableName\" % (configsTableName,pipelineGroup)\r\n",
        "\r\n",
        "\r\n",
        "dfSelect = spark.sql(sqlCommand)\r\n",
        "dfSelectCollect = dfSelect.collect()\r\n",
        "for table in dfSelectCollect:\r\n",
        "    #print(table[\"SourceChangeFeedManifestPath\"])\r\n",
        "    tableName = table[\"SourceTableName\"]\r\n",
        "    sourceManifestPath = table[\"SourceChangeFeedManifestPath\"]\r\n",
        "    \r\n",
        "    abfssString = \"abfss://\" + table[\"SourceStorageContainerName\"] + \"@\" + table[\"SourceStorageAccountName\"]  + \".dfs.core.windows.net\"\r\n",
        "    sourceManifestPath = abfssString + \"/\" + sourceManifestPath\r\n",
        "    #print(sourceManifestPath)\r\n",
        "    \r\n",
        "    revsourceManifestPath = sourceManifestPath[len(sourceManifestPath)::-1]\r\n",
        "    ordVal = revsourceManifestPath.find(\"/\")\r\n",
        "    substringrevsourceManifestPath = right(revsourceManifestPath,(len(revsourceManifestPath) -ordVal - 1))\r\n",
        "    substringsourceManifestPath = substringrevsourceManifestPath[len(substringrevsourceManifestPath)::-1]\r\n",
        "\r\n",
        "    #print(substringsourceManifestPath)\r\n",
        "    df1 = spark.read.json(sourceManifestPath, multiLine=True)\r\n",
        "    entityPaths = df1.select(\"entities\").limit(1).collect()[0][0]\r\n",
        "    \r\n",
        "    for entity in entityPaths:\r\n",
        "        \r\n",
        "        if entity[\"entityName\"]==tableName:\r\n",
        "            \r\n",
        "            entityName = entity[\"entityName\"]\r\n",
        "            entityPath = entity[\"entityPath\"]\r\n",
        "            entityManifestPath = left(entityPath,(len(entityPath) - (len(entityName)+1)))\r\n",
        "            entityConfigurationPath = substringsourceManifestPath + \"/\"+ entityManifestPath\r\n",
        "            \r\n",
        "            jsonConfigs = spark.read.json(entityConfigurationPath, multiLine=True)\r\n",
        "            definitions = jsonConfigs.select(\"definitions\").limit(1).collect()[0][0]\r\n",
        "            sqlCreate = \"CREATE OR REPLACE TABLE \" + bronzedatabaseName + \".\" + tableName + \" (\"\r\n",
        "            colList = ''\r\n",
        "            rowNum = 0\r\n",
        "            schemaList = []\r\n",
        "            for rows in definitions:\r\n",
        "                fields = rows[\"hasAttributes\"]\r\n",
        "                for field in fields:\r\n",
        "                    fieldName = field[\"name\"]\r\n",
        "                    dataFormat = field[\"dataFormat\"].replace(\"Binary\",\"String\").replace(\"DateTime\",\"Timestamp\").replace(\"Int64\",\"BIGINT\").replace(\"Int32\",\"INT\").replace(\"Decimal\",\"Decimal(32,4)\").replace(\"Guid\",\"String\")\r\n",
        "                    if rowNum == 0:\r\n",
        "                        colList = fieldName + \" \" + dataFormat\r\n",
        "                        rowNum = 1\r\n",
        "                    else:\r\n",
        "                        colList = colList + \", \" + fieldName + \" \" + dataFormat\r\n",
        "                    schemaList.append((fieldName,dataFormat))\r\n",
        "\r\n",
        "                sqlCreate = sqlCreate + colList + \", ETLBatchSeriesID BIGINT) USING DELTA\"\r\n",
        "                sqlSilverCreate = \"CREATE OR REPLACE TABLE \" + silverdatabaseName + \".\" + tableName + \" (\" + colList + \", ETLBatchSeriesID BIGINT, ETLIsDeleted INT) USING DELTA\"\r\n",
        "                #print(tableName)\r\n",
        "                spark.sql(sqlCreate) \r\n",
        "                spark.sql(sqlSilverCreate)\r\n",
        "                #print(sqlCreate)\r\n",
        "                #print(sqlSilverCreate)\r\n",
        "if pipelineGroup == 'Core':\r\n",
        "    mergeSQL = \"MERGE INTO %s.silvermeta T \\\r\n",
        "    USING ( \\\r\n",
        "    SELECT '%s' as Database, TargetTableName as TableName,concat('SELECT * FROM (SELECT *, RANK() OVER(PARTITION BY RECID ORDER BY CAST(Start_LSN as BINARY) DESC,CAST(Seq_Val as BINARY) DESC, ETLBatchSeriesID DESC) as TopRank FROM ', TargetDatabaseName, '.', TargetTableName, ' WHERE ETLBatchSeriesID >= @REPLACEBATCH@ ) a WHERE TopRank = 1') as SourceQuery,'s.`RECID` = t.`RECID`' as MergeJoinKey, IsActive, 1 as IsProcessingComplete, '%s' as PipelineGroup, CAST(0 AS BIGINT) AS ETLBatchSeriesID \\\r\n",
        "    FROM %s.bronzemetatable WHERE IsActive = 1 AND PipelineGroup = '%s' AND lower(TargetTableName) IN (%s) \\\r\n",
        "    UNION \\\r\n",
        "    SELECT '%s' as Database, 'ConstantEntity', 'SELECT TableName, FieldName, Constant0, FormDisplayName, MAX(Value) AS Value, Key, ETLBatchSeriesID, DML_Action FROM (SELECT *, RANK() OVER(PARTITION BY TableName,FieldName,Constant0,Key ORDER BY ETLBatchSeriesID DESC) as TopRank FROM %s.ConstantEntity WHERE ETLBatchSeriesID >= @REPLACEBATCH@ ) a WHERE TopRank = 1 GROUP BY TableName, FieldName, Constant0, FormDisplayName, Key, ETLBatchSeriesID, DML_Action','s.`TableName` = t.`TableName` AND s.`FieldName`= t.`FieldName` AND s.`Constant0` = t.`Constant0` and s.`Key` = t.`Key`',1, 1 as IsProcessingComplete, '%s' as PipelineGroup, CAST(0 AS BIGINT) AS ETLBatchSeriesID) AS S \\\r\n",
        "    ON T.TableName = S.TableName AND T.PipelineGroup = S.PipelineGroup \\\r\n",
        "    WHEN NOT MATCHED THEN INSERT *\" % (configsdatabaseName,silverdatabaseName,pipelineGroup,configsdatabaseName,pipelineGroup,str(tableList).replace('[','').replace(']','').lower(),silverdatabaseName,bronzedatabaseName,pipelineGroup)\r\n",
        "else:\r\n",
        "    mergeSQL = \"MERGE INTO %s.silvermeta T \\\r\n",
        "    USING ( \\\r\n",
        "    SELECT '%s' as Database, TargetTableName as TableName,concat('SELECT * FROM (SELECT *, RANK() OVER(PARTITION BY RECID ORDER BY CAST(Start_LSN as BINARY) DESC,CAST(Seq_Val as BINARY) DESC, ETLBatchSeriesID DESC) as TopRank FROM ', TargetDatabaseName, '.', TargetTableName, ' WHERE ETLBatchSeriesID >= @REPLACEBATCH@ ) a WHERE TopRank = 1') as SourceQuery,'s.`RECID` = t.`RECID`' as MergeJoinKey, IsActive, 1 as IsProcessingComplete, '%s' as PipelineGroup, CAST(0 AS BIGINT) AS ETLBatchSeriesID \\\r\n",
        "    FROM %s.bronzemetatable WHERE IsActive = 1 AND PipelineGroup = '%s' AND lower(TargetTableName) IN (%s)) S \\\r\n",
        "    ON T.TableName = S.TableName AND T.PipelineGroup = S.PipelineGroup \\\r\n",
        "    WHEN NOT MATCHED THEN INSERT *\" % (configsdatabaseName,silverdatabaseName,pipelineGroup,configsdatabaseName,pipelineGroup,str(tableList).replace('[','').replace(']','').lower())\r\n",
        "spark.sql(mergeSQL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#####################################\r\n",
        "#####   CONFIGURATION VALUES    #####\r\n",
        "#####################################\r\n",
        "if isInitialSetup != 1:\r\n",
        "    retryAttempts = 1\r\n",
        "    isFullLoad = 1\r\n",
        "    parallelWorkers = 8\r\n",
        "\r\n",
        "    df = spark.sql(\"SELECT MAX(ETLBatchSeriesID) + 1 AS ETLBatchSeriesID, COUNT(*) as RowCnt FROM %s.ETLBatchSeries WHERE PipelineGroup = '%s'\" % (configsdatabaseName,pipelineGroup))\r\n",
        "\r\n",
        "    RowCnt = df.select(\"RowCnt\").limit(1).collect()[0][0]\r\n",
        "\r\n",
        "    if RowCnt == 0:\r\n",
        "        ETLBatchSeriesID = 1\r\n",
        "        sqlQry= \"INSERT INTO %s.ETLBatchSeries (ETLBatchSeriesID,ETLBatchSeriesCreateDate,IsProcessingComplete,PipelineGroup,IsFullLoad) SELECT 1, current_timestamp(),0,'%s',%i\" % (configsdatabaseName,pipelineGroup,isFullLoad)\r\n",
        "    else:\r\n",
        "        ETLBatchSeriesID = df.select(\"ETLBatchSeriesID\").limit(1).collect()[0][0]\r\n",
        "        sqlQry= \"INSERT INTO %s.ETLBatchSeries (ETLBatchSeriesID,ETLBatchSeriesCreateDate,IsProcessingComplete,PipelineGroup,IsFullLoad) SELECT '%i', current_timestamp(),0,'%s',%i\" % (configsdatabaseName,ETLBatchSeriesID,pipelineGroup,isFullLoad)\r\n",
        "    spark.sql(sqlQry)\r\n",
        "\r\n",
        "    df = spark.sql(\"SELECT MAX(ETLBatchSeriesID) AS ETLBatchSeriesID FROM %s.ETLBatchSeries WHERE PipelineGroup = '%s'\" % (configsdatabaseName,pipelineGroup))\r\n",
        "\r\n",
        "    ETLBatchSeriesID = int(df.select(\"ETLBatchSeriesID\").limit(1).collect()[0][0])\r\n",
        "    sqlCommand = \"SELECT * FROM %s.bronzeMetaTable WHERE IsActive = 1 and lower(TargetTableName) IN (%s) AND PipelineGroup = '%s'\" % (configsdatabaseName,str(tableList).replace('[','').replace(']','').lower(),pipelineGroup)\r\n",
        "    df=spark.sql(sqlCommand)\r\n",
        "    bronzedfcollect = df.collect()   \r\n",
        "\r\n",
        "from concurrent.futures import ThreadPoolExecutor\r\n",
        "from datetime import datetime\r\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "import json\r\n",
        "resultList = []\r\n",
        "\r\n",
        "def main():\r\n",
        "    notebooks = list(gather_notebookcalls(bronzedfcollect,isFullLoad))\r\n",
        "    with ThreadPoolExecutor(max_workers=parallelWorkers) as Thready:\r\n",
        "        print(\"Thready\")\r\n",
        "        Thready.map(call_notebooks, notebooks)    \r\n",
        "\r\n",
        "def call_notebooks(notebook_par: str) -> None:\r\n",
        "    retry = retryAttempts\r\n",
        "    notebookParams = json.loads(notebook_par)\r\n",
        "    res=\"\"\r\n",
        "    try:\r\n",
        "        spark.conf.set(\"spark.synapse.nbs.kernelid\", \"\")\r\n",
        "        res = mssparkutils.notebook.run(notebookParams[\"path\"], notebookParams[\"timeout\"], notebookParams[\"parameters\"])\r\n",
        "        resultList.append(res)\r\n",
        "        print(res)\r\n",
        "        IsRetry = 0\r\n",
        "    except Exception as e:\r\n",
        "        print(e)\r\n",
        "        IsRetry = 1\r\n",
        "    if IsRetry == 1 and retry > 0:\r\n",
        "            print(\"Retrying\")\r\n",
        "            spark.conf.set(\"spark.synapse.nbs.kernelid\", \"\")\r\n",
        "            res = mssparkutils.notebook.run(notebookParams[\"path\"], notebookParams[\"timeout\"], notebookParams[\"parameters\"])\r\n",
        "            resultList.append(res)\r\n",
        "            print(res) \r\n",
        "            retry = retry - 1\r\n",
        "    else:\r\n",
        "        raise ValueError(\"ERROR DURING PROCESSING. CHECK YOUR LOGS. \" + notebookParams[\"parameters\"])\r\n",
        "    \r\n",
        "  \r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "def gather_notebookcalls(bronzedfcollect,isFullLoad) -> iter:  \r\n",
        "    retryAttempts = 1\r\n",
        "    notebooks = []\r\n",
        "    schema=StructType([ \\\r\n",
        "        StructField(\"path\",StringType(),True), \\\r\n",
        "        StructField(\"timeout\",IntegerType(),True), \\\r\n",
        "        StructField(\"parameters\",StructType([ \\\r\n",
        "            StructField(\"isFullLoad\", IntegerType(), True),  \\\r\n",
        "            StructField(\"targetDatabaseName\", StringType(), True), \\\r\n",
        "            StructField(\"targetTableName\", StringType(), True), \\\r\n",
        "            StructField(\"targetTablePath\", StringType(), True), \\\r\n",
        "            StructField(\"targetStorageContainerName\", StringType(), True), \\\r\n",
        "            StructField(\"targetStorageAccountName\", StringType(), True), \\\r\n",
        "            StructField(\"sourceStorageAccountName\", StringType(), True), \\\r\n",
        "            StructField(\"sourceStorageContainerName\", StringType(), True), \\\r\n",
        "            StructField(\"sourceManifestPath\", StringType(), True), \\\r\n",
        "            StructField(\"sourceFilePath\", StringType(), True), \\\r\n",
        "            StructField(\"checkpointAccountName\", StringType(), True), \\\r\n",
        "            StructField(\"checkpointContainerName\", StringType(), True), \\\r\n",
        "            StructField(\"checkpointPath\", StringType(), True), \\\r\n",
        "            StructField(\"mergeKey\", StringType(), True), \\\r\n",
        "            StructField(\"ETLBatchSeriesID\", IntegerType(), True)])), \\\r\n",
        "        StructField(\"retry\",StringType(),True)])\r\n",
        "    for row in bronzedfcollect:\r\n",
        "        if isFullLoad == 1:\r\n",
        "            targetDatabaseName=row['TargetDatabaseName']\r\n",
        "            targetTableName=row['TargetTableName']\r\n",
        "            targetTablePath=row['TargetTablePath']\r\n",
        "            sourceStorageContainerName=row['SourceStorageContainerName']\r\n",
        "            sourceStorageAccountName=row['SourceStorageAccountName']\r\n",
        "            sourceManifestPath=row['SourceFullTablesManifestPath']\r\n",
        "            sourceFilePath=row['SourceFullTablesFilePath']\r\n",
        "            checkpointAccountName=row['CheckpointAccountName']\r\n",
        "            checkpointContainerName=row['CheckpointContainerName']\r\n",
        "            checkpointPath=row['FullCheckpointPath']\r\n",
        "            targetStorageContainerName=row['TargetStorageContainerName']\r\n",
        "            targetStorageAccountName=row['TargetStorageAccountName']\r\n",
        "            mergeKey=\"1=2\"\r\n",
        "        else:\r\n",
        "            targetDatabaseName=row['TargetDatabaseName']\r\n",
        "            targetTableName=row['TargetTableName']\r\n",
        "            targetTablePath=row['TargetTablePath']\r\n",
        "            sourceStorageContainerName=row['SourceStorageContainerName']\r\n",
        "            sourceStorageAccountName=row['SourceStorageAccountName']\r\n",
        "            sourceManifestPath=row['SourceChangeFeedManifestPath']\r\n",
        "            sourceFilePath=row['SourceChangeFeedFilePath']\r\n",
        "            checkpointAccountName=row['CheckpointAccountName']\r\n",
        "            checkpointContainerName=row['CheckpointContainerName']\r\n",
        "            checkpointPath=row['ChangeFeedCheckpointPath']\r\n",
        "            targetStorageContainerName=row['TargetStorageContainerName']\r\n",
        "            targetStorageAccountName=row['TargetStorageAccountName']\r\n",
        "            mergeKey=\"s.recid=t.recid\"\r\n",
        "        notebooks.append((\"/StgToBronze\",5000,(isFullLoad,targetDatabaseName,targetTableName,targetTablePath,targetStorageContainerName,targetStorageAccountName,sourceStorageAccountName,sourceStorageContainerName,sourceManifestPath,sourceFilePath,checkpointAccountName,checkpointContainerName,checkpointPath,mergeKey,ETLBatchSeriesID),retryAttempts))\r\n",
        "    rdd = sc.parallelize(notebooks)\r\n",
        "    dfBronzeConfigurations = sqlContext.createDataFrame(rdd, schema)  \r\n",
        "    notebookCollect = dfBronzeConfigurations.toJSON().collect()\r\n",
        "    for notebook in notebookCollect:\r\n",
        "        yield notebook\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    t1 = datetime.now()\r\n",
        "    if isInitialSetup != 1:\r\n",
        "        main()\r\n",
        "    t2 = datetime.now()\r\n",
        "    x = t2-t1\r\n",
        "    print(f'It took {x} to process files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "updateBronzeMeta = \"UPDATE %s.bronzemetatable \\\r\n",
        "SET ETLBatchSeriesID = %s \\\r\n",
        "WHERE IsActive = 1 and lower(TargetTableName) IN (%s) AND PipelineGroup = '%s'\" % (configsdatabaseName,str(ETLBatchSeriesID),str(tableList).replace('[','').replace(']','').lower(),pipelineGroup)\r\n",
        "\r\n",
        "spark.sql(updateBronzeMeta)\r\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\r\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, LongType\r\n",
        "if isInitialSetup != 1:\r\n",
        "    if pipelineGroup == \"Core\":\r\n",
        "        dfConfigurations = spark.sql(\"SELECT * FROM %s.bronzeMetaTable WHERE IsActive = 1 AND PipelineGroup = 'Core'\" % (configsdatabaseName))\r\n",
        "        dfConfigurationsCollect = dfConfigurations.collect()\r\n",
        "        enumValues = []    \r\n",
        "        DMLAction = \"INSERT\"\r\n",
        "        for table in dfConfigurationsCollect:\r\n",
        "            tableName = table[\"SourceTableName\"]\r\n",
        "            abfssString = \"abfss://\" + table[\"SourceStorageContainerName\"] + \"@\" + table[\"SourceStorageAccountName\"]  + \".dfs.core.windows.net\"\r\n",
        "\r\n",
        "            sourceManifestPath = table[\"SourceChangeFeedManifestPath\"]\r\n",
        "            sourceFilePath = table[\"SourceChangeFeedFilePath\"]\r\n",
        "\r\n",
        "            sourceManifestPath = abfssString + \"/\" + sourceManifestPath\r\n",
        "            \r\n",
        "            revsourceManifestPath = sourceManifestPath[len(sourceManifestPath)::-1]\r\n",
        "            ordVal = revsourceManifestPath.find(\"/\")\r\n",
        "            substringrevsourceManifestPath = right(revsourceManifestPath,(len(revsourceManifestPath) -ordVal - 1))\r\n",
        "            substringsourceManifestPath = substringrevsourceManifestPath[len(substringrevsourceManifestPath)::-1]\r\n",
        "            df1 = spark.read.json(sourceManifestPath, multiLine=True)\r\n",
        "            entityPaths = df1.select(\"entities\").limit(1).collect()[0][0]\r\n",
        "            \r\n",
        "            for entity in entityPaths:        \r\n",
        "                if entity[\"entityName\"]==tableName:\r\n",
        "\r\n",
        "                    entityName = entity[\"entityName\"]\r\n",
        "                    entityPath = entity[\"entityPath\"]\r\n",
        "                    entityManifestPath = left(entityPath,(len(entityPath) - (len(entityName)+1)))\r\n",
        "                    entityConfigurationPath = substringsourceManifestPath + \"/\"+ entityManifestPath\r\n",
        "                    jsonConfigs = spark.read.json(entityConfigurationPath, multiLine=True)\r\n",
        "                    definitions = jsonConfigs.select(\"definitions\").limit(1).collect()[0][0]           \r\n",
        "                    \r\n",
        "                    \r\n",
        "                    for rows in definitions:\r\n",
        "                        \r\n",
        "                        numAttributes = len(rows['hasAttributes'])\r\n",
        "                        \r\n",
        "                        loopCount = 0\r\n",
        "                        while loopCount < numAttributes:\r\n",
        "                            try:\r\n",
        "                                constantValues = json.loads(rows['hasAttributes'][loopCount]['appliedTraits'][3])\r\n",
        "                                fieldName = rows['hasAttributes'][loopCount]['name']\r\n",
        "                                #print((rows['hasAttributes'][5]['appliedTraits']))\r\n",
        "                                loopCount = loopCount+1\r\n",
        "                                rowCount = len(constantValues['arguments'][1]['value']['entityReference']['constantValues'])\r\n",
        "                                i = 0\r\n",
        "                                while i < rowCount:\r\n",
        "                                    unumList = constantValues['arguments'][1]['value']['entityReference']['constantValues'][i]\r\n",
        "                                    unumList.insert(0,tableName)\r\n",
        "                                    unumList.insert(1,fieldName)\r\n",
        "                                    unumList.append(ETLBatchSeriesID)\r\n",
        "                                    unumList.append(DMLAction)\r\n",
        "                                    enumValues.append(unumList)\r\n",
        "                                    i=i+1\r\n",
        "                            except:\r\n",
        "                                loopCount = loopCount + 1\r\n",
        "\r\n",
        "        schema = StructType([ \\\r\n",
        "            StructField(\"TableName\",StringType(),True), \\\r\n",
        "            StructField(\"FieldName\",StringType(),True), \\\r\n",
        "            StructField(\"Constant0\",StringType(),True), \\\r\n",
        "            StructField(\"FormDisplayName\",StringType(),True), \\\r\n",
        "            StructField(\"Value\",StringType(),True), \\\r\n",
        "            StructField(\"Key\",StringType(),True), \\\r\n",
        "            StructField(\"ETLBatchSeriesID\",LongType(),True),\r\n",
        "            StructField(\"DML_Action\",StringType(),True) ])\r\n",
        "\r\n",
        "\r\n",
        "        rdd = sc.parallelize(enumValues)\r\n",
        "        dfFullBronzeConfigurations = sqlContext.createDataFrame(rdd, schema)  \r\n",
        "\r\n",
        "        dfFullBronzeConfigurations.createOrReplaceTempView(\"vwConstantEntity\")\r\n",
        "        dfFullBronzeConfigurations = spark.sql(\"SELECT DISTINCT * FROM vwConstantEntity\")\r\n",
        "        sqlCommand = \"MERGE INTO \" + bronzedatabaseName + \".ConstantEntity T \\\r\n",
        "        USING vwConstantEntity S \\\r\n",
        "        ON T.TableName = S.TableName AND T.FieldName = S.FieldName AND S.Constant0 = T.Constant0 AND S.FormDisplayName = T.FormDisplayName AND S.Value = T.Value AND S.Key = T.Key \\\r\n",
        "        WHEN NOT MATCHED THEN INSERT *\"\r\n",
        "        spark.sql(sqlCommand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if isInitialSetup != 1:\r\n",
        "    df=spark.sql(\"SELECT REPLACE(SourceQuery,'@REPLACEBATCH@',\"+str(ETLBatchSeriesID)+\") AS SourceQuery, TableName, MergeJoinKey, Database FROM %s.SilverMeta WHERE IsActive = 1 and lower(TableName) IN (%s,'constantentity') AND PipelineGroup = '%s'\" % (configsdatabaseName,str(tableList).replace('[','').replace(']','').lower(),pipelineGroup))\r\n",
        "    silverdfcollect = df.collect()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\r\n",
        "from datetime import datetime\r\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
        "import json\r\n",
        "resultList = []\r\n",
        "def main():\r\n",
        "    notebooks = list(gather_notebookcalls(silverdfcollect,isFullLoad))\r\n",
        "    with ThreadPoolExecutor(max_workers=parallelWorkers) as Thready:\r\n",
        "        #print(\"Thready\")\r\n",
        "        Thready.map(call_notebooks, notebooks)    \r\n",
        "        \r\n",
        "        \r\n",
        "\"\"\"def call_notebooks(notebook_par: str) -> None:\r\n",
        "    notebookParams = json.loads(notebook_par)\r\n",
        "    #print(notebookParams[\"parameters\"])\r\n",
        "    res = mssparkutils.notebook.run(notebookParams[\"path\"], notebookParams[\"timeout\"], notebookParams[\"parameters\"])\r\n",
        "    resultList.append(res)\"\"\"\r\n",
        "\r\n",
        "def call_notebooks(notebook_par: str) -> None:\r\n",
        "    retry = retryAttempts\r\n",
        "    notebookParams = json.loads(notebook_par)\r\n",
        "    #print(notebookParams[\"parameters\"])\r\n",
        "    res=\"\"\r\n",
        "    try:\r\n",
        "        spark.conf.set(\"spark.synapse.nbs.kernelid\", \"\")\r\n",
        "        res = mssparkutils.notebook.run(notebookParams[\"path\"], notebookParams[\"timeout\"], notebookParams[\"parameters\"])\r\n",
        "        resultList.append(res)\r\n",
        "        print(res)\r\n",
        "        IsRetry = 0\r\n",
        "    except Exception as e:\r\n",
        "        print(e)\r\n",
        "        IsRetry = 1\r\n",
        "    if IsRetry == 1 and retry > 0:\r\n",
        "            print(\"Retrying\")\r\n",
        "            spark.conf.set(\"spark.synapse.nbs.kernelid\", \"\")\r\n",
        "            res = mssparkutils.notebook.run(notebookParams[\"path\"], notebookParams[\"timeout\"], notebookParams[\"parameters\"])\r\n",
        "            resultList.append(res)\r\n",
        "            print(res) \r\n",
        "            retry = retry - 1\r\n",
        "    else:\r\n",
        "        raise ValueError(\"ERROR DURING PROCESSING. CHECK YOUR LOGS. \" + notebookParams[\"parameters\"])\r\n",
        "\r\n",
        "def gather_notebookcalls(silverdfcollect,isFullLoad) -> iter:  \r\n",
        "    retryAttempts = 1\r\n",
        "    notebooks = []\r\n",
        "    #isFullLoad = 0\r\n",
        "    schema = StructType([ \\\r\n",
        "        StructField(\"path\",StringType(),True), \\\r\n",
        "        StructField(\"timeout\",IntegerType(),True), \\\r\n",
        "        StructField(\"parameters\",StructType([ \\\r\n",
        "            StructField(\"isFullLoad\", IntegerType(), True),  \\\r\n",
        "            StructField(\"databaseName\", StringType(), True), \\\r\n",
        "            StructField(\"tableName\", StringType(), True), \\\r\n",
        "            StructField(\"ETLBatchSeriesID\", StringType(), True), \\\r\n",
        "            StructField(\"mergeKey\", StringType(), True), \\\r\n",
        "            StructField(\"SourceQuery\", StringType(), True)])), \\\r\n",
        "        StructField(\"retry\",StringType(),True)])\r\n",
        "\r\n",
        "    for row in silverdfcollect:\r\n",
        "        tableName = row['TableName']\r\n",
        "        MergeJoinKey = str(row['MergeJoinKey'])\r\n",
        "        sourceQuery = row['SourceQuery']\r\n",
        "        notebooks.append((\"/BronzeToSilver\",5000,(isFullLoad,row['Database'],row['TableName'],ETLBatchSeriesID,MergeJoinKey,sourceQuery),retryAttempts)) \r\n",
        "    \r\n",
        "    rdd = sc.parallelize(notebooks)\r\n",
        "    dfSilverConfigurations = sqlContext.createDataFrame(rdd, schema)  \r\n",
        "    notebookCollect = dfSilverConfigurations.toJSON().collect()\r\n",
        "    for notebook in notebookCollect:\r\n",
        "        yield notebook\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    t1 = datetime.now()\r\n",
        "    if isInitialSetup != 1:\r\n",
        "        main()\r\n",
        "    t2 = datetime.now()\r\n",
        "    x = t2-t1\r\n",
        "    print(f'It took {x} to process files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "updateSilverMeta = \"UPDATE %s.silvermeta \\\r\n",
        "SET ETLBatchSeriesID = %s \\\r\n",
        "WHERE IsActive = 1 and lower(TableName) IN (%s,'constantentity') AND PipelineGroup = '%s'\" % (configsdatabaseName,str(ETLBatchSeriesID),str(tableList).replace('[','').replace(']','').lower(),pipelineGroup)\r\n",
        "\r\n",
        "spark.sql(updateSilverMeta)\r\n",
        "\r\n",
        "if isInitialSetup != 1:\r\n",
        "    updateBatch = \"UPDATE %s.etlbatchseries \\\r\n",
        "    SET IsProcessingComplete = 1 \\\r\n",
        "    WHERE PipelineGroup = '%s' \\\r\n",
        "    AND ETLBatchSeriesID = %i\" % (configsdatabaseName,pipelineGroup,ETLBatchSeriesID)\r\n",
        "    spark.sql(updateBatch)\r\n",
        ""
      ]
    }
  ]
}