import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.linear_model import LassoCV, Ridge
from sklearn.metrics import mean_squared_error, r2_score

# Sample data and parameters
products = ['Product_A', 'Product_B', 'Product_C']
num_dates = 10
num_lags = 5

# Function to calculate goodness of fit (R-squared)
def goodness_of_fit(y_true, y_pred):
    return r2_score(y_true, y_pred)

# Create an empty DataFrame to store results
results_df = pd.DataFrame(columns=['Actual', 'Predicted', 'RMSE', 'Difference', 'Goodness_of_Fit'])

# Loop through each product
for product in products:
    # Extract the time series DataFrame for the current product using multi-indexing
    product_df = final_df.loc[product]
    
    # Split data into train and test sets
    X = product_df.iloc[:, :-1]  # Features (lags)
    y = product_df.iloc[:, -1]   # Target variable
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Create a pipeline for modeling
    pipeline = Pipeline([
        ('model', RandomForestRegressor()),  # Replace with different models
    ])
    
    # Fit the pipeline on the training data
    pipeline.fit(X_train, y_train)
    
    # Make predictions on the test data
    y_pred = pipeline.predict(X_test)
    
    # Calculate RMSE
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    # Calculate difference between actual and predicted
    diff = y_test - y_pred
    
    # Calculate goodness of fit (R-squared)
    gof = goodness_of_fit(y_test, y_pred)
    
    # Store results in the final DataFrame
    results_df.loc[product] = [y_test.values, y_pred, rmse, diff, gof]

# Now, results_df contains the results for all products
print(results_df)
